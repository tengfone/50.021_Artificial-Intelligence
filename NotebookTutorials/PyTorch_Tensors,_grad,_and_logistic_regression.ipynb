{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of week 5 - AI class - PyTorch - Tensors, grad, and logistic regression",
      "provenance": [],
      "collapsed_sections": [
        "zCMO9UqgVkS1",
        "KdMeqnLiovOk"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NAyKlyGxHT9"
      },
      "source": [
        "# PyTorch Tutorial - Tensors, grad, and logistic regression\n",
        "\n",
        "Prof. Dorien Herremans, with many thanks to Nelson Lui for the base text. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZnM_yfCBz6P"
      },
      "source": [
        "**To edit the notebook**:\n",
        "\n",
        "There are two ways to edit the notebook.\n",
        "\n",
        "You can either open it in the \"playground\", where you can change and run cells. After closing the tab, your changes will be lost. To do so, press \"File\" > \"Open in playground\".\n",
        "\n",
        "Alternatively, you can make a copy of this notebook to your own Google Drive account through \"File\" > \"Save a copy in Drive...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK-sJIRG6BLp"
      },
      "source": [
        "**Activating the GPU on Colab**:\n",
        "\n",
        "Colab now gives you 12 hours of free GPU time (before you have to request a new node).\n",
        "Simply select \"GPU\" in the Accelerator drop-down in Notebook Settings (either through the Edit menu or the command palette at cmd/ctrl-shift-P)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgs23T03xBK7"
      },
      "source": [
        "# Setting up the notebook on colab\n",
        "\n",
        "Let's check if we are using the GPU environment and cuda is installed: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kig3C9d9D-kA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1e13c0-22e0-4ab9-90ec-01d5de9d3c7b"
      },
      "source": [
        "# Import PyTorch and other libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"PyTorch version:\")\n",
        "print(torch.__version__)\n",
        "print(\"GPU Detected:\")\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version:\n",
            "1.8.1+cu101\n",
            "GPU Detected:\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAQvK5xHFe5I"
      },
      "source": [
        "# What is PyTorch?\n",
        "\n",
        "PyTorch is a deep learning package for building dynamic computation graphs.\n",
        "\n",
        "More broadly, it's a GPU-compatible replacement for NumPy. You can think of it as NumPy + auto-differentiation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O02f3GJILvwJ"
      },
      "source": [
        "# Basic Mechanics\n",
        "\n",
        "If you are interested in basic operations, please go through the official PyTorch tutorial on tensors here: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf9cLN-1GCao"
      },
      "source": [
        "## Tensors\n",
        "The `Tensor` type is essentially a NumPy `ndarray`. However, `Tensors` can critically be moved to the GPU for accelerated computing.\n",
        "\n",
        "There are several types of `Tensors`, each of which correspond to a NumPy `dtype` and whether it is on the CPU or GPU.\n",
        "\n",
        "The main ones you will probably use are:\n",
        "\n",
        "| Data Type | CPU Tensor Type | GPU Tensor Type | NumPy dtype\n",
        "| --- | --- | --- | --- | \n",
        "| 32-bit floating point | `torch.FloatTensor` | `torch.cuda.FloatTensor` | `float32` |\n",
        "| 8-bit integer (unsigned) | `torch.ByteTensor` | `torch.cuda.ByteTensor` | `uint8` |\n",
        "| 64-bit integer (signed)  | `torch.LongTensor` | `torch.cuda.LongTensor` | `int64` |\n",
        "\n",
        "In general, you want to use `FloatTensor` by default, unless your data is specifically an integer (in which case you'd use a `LongTensor`) or your data is bits (you'd want to use `ByteTensor`).\n",
        "\n",
        "You can find a full list of tensor types [here](http://pytorch.org/docs/master/tensors.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTQXFp7sGSYE"
      },
      "source": [
        "To construct a uninitialized 4x6 matrix (think `malloc` for those of you familar with `C` language, so not guaranteed to be all `0`), we can use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXpJJ0xfGQN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b44f8dc-6aea-4b31-fe9e-bd6321a472fb"
      },
      "source": [
        "# Note that torch.Tensor is short for torch.FloatTensor\n",
        "uninit_float = torch.Tensor(4, 6)\n",
        "print(uninit_float)\n",
        "print(\"Type of above Tensor (it's also printed when you print the tensor):\")\n",
        "print(type(uninit_float))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.8783e+34, 3.0686e-41, 7.0065e-44, 6.8664e-44, 6.3058e-44, 6.7262e-44],\n",
            "        [7.5670e-44, 6.3058e-44, 6.7262e-44, 6.8664e-44, 1.1771e-43, 6.7262e-44],\n",
            "        [7.1466e-44, 8.1275e-44, 7.4269e-44, 6.8664e-44, 8.1275e-44, 6.7262e-44],\n",
            "        [7.5670e-44, 6.4460e-44, 7.9874e-44, 6.7262e-44, 7.2868e-44, 7.4269e-44]])\n",
            "Type of above Tensor (it's also printed when you print the tensor):\n",
            "<class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuWOtvMQXvQH"
      },
      "source": [
        "We can also create Tensors directly from (optionally nested) lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_HlWP46XulR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2eb70c8-f546-4138-d3e8-c21b2628b21d"
      },
      "source": [
        "some_float_tensor = torch.Tensor([3.2, 4.3, 5.5])\n",
        "print(some_float_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3.2000, 4.3000, 5.5000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e11zDwHKIyso"
      },
      "source": [
        "If we want a random uniform initialized `FloatTensor`, we can use `rand`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ihZMTIQIyHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6f6801-23a9-40b6-b1ce-eba7adfc9957"
      },
      "source": [
        "rand_float = torch.rand(4, 6)\n",
        "print(rand_float)\n",
        "print(type(rand_float))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2706, 0.5993, 0.8612, 0.1359, 0.9041, 0.2781],\n",
            "        [0.1394, 0.6222, 0.2020, 0.1138, 0.3147, 0.1496],\n",
            "        [0.2429, 0.2838, 0.4365, 0.5322, 0.9875, 0.2481],\n",
            "        [0.1620, 0.5209, 0.6757, 0.2193, 0.0877, 0.0500]])\n",
            "<class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJhXiTl8KAAH"
      },
      "source": [
        "Let's print the `shape` of our random tensor. In NumPy / PyTorch / other tensor-manipulation libraries, `shape` refers to the dimensions of the tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7TvFs-9K4Ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c443f70-6bde-4614-a85d-e9c379107392"
      },
      "source": [
        "# Get the size of the rand float\n",
        "print(rand_float.size())\n",
        "\n",
        "# What's this weird torch.Size datatype?\n",
        "print(type(rand_float.size()))\n",
        "print()\n",
        "\n",
        "# It's just a tuple!\n",
        "print(\"Is rand_float.size() a tuple?\")\n",
        "print(isinstance(rand_float.size(), tuple))\n",
        "print()\n",
        "\n",
        "# We can even extract specific dimensions.\n",
        "# The two lines below are functionally identical.\n",
        "print(\"Size of rand_float dimension 1:\")\n",
        "print(rand_float.size()[0])\n",
        "print(rand_float.size(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 6])\n",
            "<class 'torch.Size'>\n",
            "\n",
            "Is rand_float.size() a tuple?\n",
            "True\n",
            "\n",
            "Size of rand_float dimension 1:\n",
            "4\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx_j18ospHiv"
      },
      "source": [
        "## NumPy Bridge\n",
        "\n",
        "It's very easy to convert a NumPy array into a Torch Tensor and vice versa as they will share their underlying memory locations (if the tensor is on CPU). Note that changing one will change the other.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POznDR56pWc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6c80ddf-34ea-4c5a-fe06-c86e62b372c7"
      },
      "source": [
        "a = torch.ones(6)\n",
        "print (a)\n",
        "\n",
        "b = a.numpy()\n",
        "print (b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1.])\n",
            "[1. 1. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56bnT-KqplKG"
      },
      "source": [
        "Notice how they share the same memory: you change 1 and the other changes as well: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyQnMzvPpowY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f6f494d-1c02-4923-d574-5b070ec0aee7"
      },
      "source": [
        "a.add_(4)\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5., 5., 5., 5., 5., 5.])\n",
            "[5. 5. 5. 5. 5. 5.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxQTPkDRpxQ7"
      },
      "source": [
        "We can just as easily convert the other way, and once again observe the same memory sharing behaviour: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1K6CHnkpzsN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa85ab60-50db-4de3-eb7e-963ff721f288"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.ones(6)\n",
        "b = torch.from_numpy(a)\n",
        "\n",
        "np.add(a, 1, out=a)\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2. 2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gpjuU8cMhkP"
      },
      "source": [
        "## Operations\n",
        "\n",
        "PyTorch has a huge library of various operations (e.g. indexing, slicing, math, linear algebra, sampling, etc). They're all listed [here](http://pytorch.org/docs/0.3.1/torch.html). We'll experiment with the addition operation below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snlhjN6pPLLs"
      },
      "source": [
        "We can add with the normal Python `+` operator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSbDAqt1NX3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e3125da-f7f8-4f30-90f7-5fe9c3146b17"
      },
      "source": [
        "other_rand_float = torch.rand(4, 6)\n",
        "# Three ways to add!\n",
        "\n",
        "# Python Operator +\n",
        "print(rand_float + other_rand_float)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6864, 0.7006, 1.2178, 0.3397, 1.4092, 0.8570],\n",
            "        [0.6551, 1.4517, 0.7089, 0.3275, 0.8564, 0.3426],\n",
            "        [0.7976, 0.5284, 1.0883, 1.0456, 1.0361, 0.5432],\n",
            "        [1.1440, 0.6275, 1.0399, 1.0930, 0.1585, 0.4573]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFcboDXrPULm"
      },
      "source": [
        "We can also use the `torch.add` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzfAtigCPUUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2346a206-5c1c-4199-94e0-988adac26eab"
      },
      "source": [
        "# torch.add\n",
        "print(torch.add(rand_float, other_rand_float))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6864, 0.7006, 1.2178, 0.3397, 1.4092, 0.8570],\n",
            "        [0.6551, 1.4517, 0.7089, 0.3275, 0.8564, 0.3426],\n",
            "        [0.7976, 0.5284, 1.0883, 1.0456, 1.0361, 0.5432],\n",
            "        [1.1440, 0.6275, 1.0399, 1.0930, 0.1585, 0.4573]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFuv7i7GPUgo"
      },
      "source": [
        "We can also add in-place to `rand_float`. This modifies the `rand_float` tensor. All PyTorch operations that modify in place end with an underscore (\"_\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X3Mzao-PUqX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9e15c45-898a-406a-9988-818db4027785"
      },
      "source": [
        "# Add in-place to rand_float. This modifies rand_float!\n",
        "rand_float.add_(other_rand_float)\n",
        "print(rand_float)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6864, 0.7006, 1.2178, 0.3397, 1.4092, 0.8570],\n",
            "        [0.6551, 1.4517, 0.7089, 0.3275, 0.8564, 0.3426],\n",
            "        [0.7976, 0.5284, 1.0883, 1.0456, 1.0361, 0.5432],\n",
            "        [1.1440, 0.6275, 1.0399, 1.0930, 0.1585, 0.4573]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptlr7artP5Pr"
      },
      "source": [
        "## Broadcasting\n",
        "\n",
        "Broadcasting is a construct in NumPy and PyTorch that lets operations apply to tensors of different shapes. Under certain conditions, a smaller tensor can be \"broadcast\" across a bigger one. This is often desirable to do, since the looping happens at the C-level and is _incredibly_ efficient in both speed and memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpySOGBqQ-I-"
      },
      "source": [
        "In the example below, `x` has shape `(3,)` and y has shape `(5, 3)`. We can still add them together --- the smaller tensor is automatically added to each row of the larger tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mVSCqooQWzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b13c74c-5c8f-4b50-f920-ae496b9f3767"
      },
      "source": [
        "# Random LongTensors from 0 to 9.\n",
        "x = torch.LongTensor(3).random_(10)\n",
        "y = torch.LongTensor(5, 3).random_(10)\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(x+y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2, 6, 0])\n",
            "tensor([[8, 5, 8],\n",
            "        [0, 1, 7],\n",
            "        [7, 4, 1],\n",
            "        [5, 5, 5],\n",
            "        [8, 7, 4]])\n",
            "tensor([[10, 11,  8],\n",
            "        [ 2,  7,  7],\n",
            "        [ 9, 10,  1],\n",
            "        [ 7, 11,  5],\n",
            "        [10, 13,  4]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuW86QzX1rbi"
      },
      "source": [
        "**Broadcasting, if used improperly, can also lead to inadvertent bugs**. \n",
        "\n",
        "Consider this example: Say you want to multiply a matrix of shape `(4, 6)` with one of shape `(6, 4)` to get something of shape `(4, 4)`. You might be tempted to use the `*` operator, which is for `elementwise` multiplication. For matrix multiplication, we use either `Tensor.mm` or the `@` operator.\n",
        "\n",
        "However, broadcasting leads to a particularly nasty bug that is hard to detect due to broadcast (this behavior is thankfully being deprecated by PyTorch, hence you will see a bug when you run this in a recent version!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNXXI_Ti2OSl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "3692de91-0be4-4eba-f752-d26847461481"
      },
      "source": [
        "x = torch.LongTensor(4, 6).random_(10)  # [4,6]\n",
        "y = torch.LongTensor(6, 4).random_(10)  # [6,4]\n",
        "\n",
        "print(\"x: \", x)\n",
        "print(\"y: \", y)\n",
        "\n",
        "# Matrix multiply\n",
        "print(\"x @ y (matrix multiply) : \", x @ y)\n",
        "\n",
        "# USUALLY UNINTENTIONAL ELEMENTWISE-MULTIPLICATION\n",
        "print(\"x * y (elementwise multiply) : \", x * y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:  tensor([[7, 3, 7, 6, 4, 4],\n",
            "        [1, 9, 8, 5, 8, 6],\n",
            "        [6, 9, 4, 6, 9, 6],\n",
            "        [9, 9, 1, 0, 9, 6]])\n",
            "y:  tensor([[8, 2, 5, 3],\n",
            "        [6, 9, 8, 0],\n",
            "        [4, 4, 2, 9],\n",
            "        [5, 3, 0, 2],\n",
            "        [8, 1, 8, 2],\n",
            "        [0, 9, 4, 4]])\n",
            "x @ y (matrix multiply) :  tensor([[164, 127, 121, 120],\n",
            "        [183, 192, 181, 125],\n",
            "        [220, 190, 206, 108],\n",
            "        [202, 166, 215,  78]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-85ac7ff08227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# USUALLY UNINTENTIONAL ELEMENTWISE-MULTIPLICATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x * y (elementwise multiply) : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KVSYJr622sB"
      },
      "source": [
        "A big part of programming with tensors is keeping track of the expected shapes of your tensors and whether these shapes are actually showing up --- doing so will dramatically reduce the amount of bugs you have."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dniKPStURqRd"
      },
      "source": [
        "## Reshaping\n",
        "\n",
        "It's often desirable to reshape a Tensor, maybe to broadcast with something else or to turn it into something that is easier to reason about.\n",
        "We can do that with the `.view` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c49vanzR-FZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe7305a-e63a-41d5-d1a2-c4c8dd5c18b1"
      },
      "source": [
        "x = torch.LongTensor(4, 4).random_(10)\n",
        "print(x)\n",
        "\n",
        "# Turn it into a Tensor of shape (2, 8)\n",
        "y = x.view(2, 8)\n",
        "print(y)\n",
        "\n",
        "# Turn it into a Tensor of shape (8, ?).\n",
        "# The -1 is inferred from the shape of the Tensor.\n",
        "z = x.view(8, -1)\n",
        "print(z)\n",
        "\n",
        "# Turn it into a Tensor of shape (16,) (flatten it).\n",
        "# This is the same as x.view(16).\n",
        "flat = x.view(-1)\n",
        "print(flat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[7, 2, 0, 5],\n",
            "        [5, 0, 0, 6],\n",
            "        [4, 0, 8, 2],\n",
            "        [0, 3, 4, 0]])\n",
            "tensor([[7, 2, 0, 5, 5, 0, 0, 6],\n",
            "        [4, 0, 8, 2, 0, 3, 4, 0]])\n",
            "tensor([[7, 2],\n",
            "        [0, 5],\n",
            "        [5, 0],\n",
            "        [0, 6],\n",
            "        [4, 0],\n",
            "        [8, 2],\n",
            "        [0, 3],\n",
            "        [4, 0]])\n",
            "tensor([7, 2, 0, 5, 5, 0, 0, 6, 4, 0, 8, 2, 0, 3, 4, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCMO9UqgVkS1"
      },
      "source": [
        "## Slicing and Indexing\n",
        "\n",
        "PyTorch follows the same conventions that NumPy uses for array slicing and indexing. [Here's a good intro to slicing and indexing in NumPy](http://www.scipy-lectures.org/intro/numpy/array_object.html#indexing-and-slicing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlX0WJ9bWMGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed78a5aa-f6ed-4407-885f-7afc6e0ca333"
      },
      "source": [
        "x = torch.LongTensor(3, 5).random_(10)\n",
        "print(x)\n",
        "\n",
        "# Get the first row\n",
        "print(\"First row:\")\n",
        "print(x[0])\n",
        "\n",
        "# Get the last row\n",
        "print(\"Last row:\")\n",
        "print(x[-1])\n",
        "\n",
        "# Get the 3rd column\n",
        "print(\"3rd column from left:\")\n",
        "print(x[:, 2])\n",
        "\n",
        "# Get the last column\n",
        "print(\"Last column from left:\")\n",
        "print(x[:, -1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 2, 5, 7, 2],\n",
            "        [9, 3, 3, 9, 1],\n",
            "        [4, 7, 3, 4, 4]])\n",
            "First row:\n",
            "tensor([0, 2, 5, 7, 2])\n",
            "Last row:\n",
            "tensor([4, 7, 3, 4, 4])\n",
            "3rd column from left:\n",
            "tensor([5, 3, 3])\n",
            "Last column from left:\n",
            "tensor([2, 1, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "198ovrfWWkx6"
      },
      "source": [
        "Here's a slightly more complex example with a 3D Tensor --- slicing an indexing a 3D tensor is quite common in neural NLP, especially when dealing with the output of a recurrent neural network (RNN). The same slicing principles apply, though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAqxtUO-WvnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab4e1ba-5882-4fd4-cadb-5cb3c187124b"
      },
      "source": [
        "# Shape of x is (batch_size, sequence_length, hidden_dim)\n",
        "# 3 is the batch size.\n",
        "# 5 is the sequence length of all examples in the batch.\n",
        "# 10 is the size of the RNN hidden state.\n",
        "x = torch.LongTensor(3, 5, 10).random_(15)\n",
        "print(x)\n",
        "\n",
        "# Get the last LSTM outputs for each example in the batch\n",
        "print(\"Final LSTM outputs for each example: \")\n",
        "print(x[:, -1, :])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[14,  1,  4,  9,  0, 10, 12,  2, 12, 12],\n",
            "         [ 3, 11, 14,  1,  5,  2,  6,  1, 14, 12],\n",
            "         [11, 12,  0,  6, 12,  9,  6,  1,  5,  1],\n",
            "         [ 2,  5,  8, 11, 13,  7, 10,  9,  1,  4],\n",
            "         [ 2, 10,  3,  0, 11,  7,  0,  7,  0,  6]],\n",
            "\n",
            "        [[ 5, 13, 12,  0,  1,  9,  2,  0,  4,  6],\n",
            "         [ 0,  8,  8,  9,  4,  6, 10,  4, 11,  0],\n",
            "         [11,  6, 14, 13,  6,  1,  2,  7,  1,  9],\n",
            "         [ 0,  8,  7,  7,  3, 10,  3,  7,  2,  3],\n",
            "         [ 6, 11, 14,  2, 11, 12, 10,  0,  7,  9]],\n",
            "\n",
            "        [[ 8,  3,  9, 13, 10, 12,  4,  3, 13, 11],\n",
            "         [ 6,  4,  7, 13,  0,  9,  6, 14, 12,  9],\n",
            "         [14, 11,  8,  9, 12,  6,  2, 13, 12,  7],\n",
            "         [ 7,  0, 11,  7,  8,  2,  2, 14,  9,  2],\n",
            "         [ 7,  5, 11, 14,  2,  5, 12, 10,  1,  5]]])\n",
            "Final LSTM outputs for each example: \n",
            "tensor([[ 2, 10,  3,  0, 11,  7,  0,  7,  0,  6],\n",
            "        [ 6, 11, 14,  2, 11, 12, 10,  0,  7,  9],\n",
            "        [ 7,  5, 11, 14,  2,  5, 12, 10,  1,  5]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdMeqnLiovOk"
      },
      "source": [
        "# Using the GPU\n",
        "\n",
        "PyTorch allows you to easily move computations to the GPU --- just move the associated input tensors to the GPU with the `.cuda()` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76uMYna9o_2K"
      },
      "source": [
        "Note that GPU and CPU tensors are fundamentally different types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYYELXQ5o7tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab89cc21-e516-43ef-a23a-ccb139956ae8"
      },
      "source": [
        "if torch.cuda.is_available:\n",
        "  # Create a Tensor\n",
        "  x = torch.rand(3, 5)\n",
        "  print(type(x))\n",
        "\n",
        "  # Move it to the GPU\n",
        "  x_gpu = x.cuda()\n",
        "  print(type(x_gpu))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjqQ4wLIp52S"
      },
      "source": [
        "If you're using a machine with a GPU, you can run `nvidia-smi` in bash to get GPU usage statistics. Below, you can see the type of GPU, the current memory usage, the amount of memory the GPU has, and the % of the GPU being used for computatation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xchx1H8p32n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "353223f9-7964-41a3-daec-72da50b2639e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jun  1 03:54:12 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    32W / 250W |    897MiB / 16280MiB |      2%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ur47t88ubC9"
      },
      "source": [
        "Let's test out the GPU with a large matrix multiply!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um3FePM7uj14"
      },
      "source": [
        "# Some test inputs\n",
        "test_input_one = torch.rand(1000, 9000)\n",
        "test_input_two = torch.rand(9000, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmdoOmS0vDWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c5fcc45-476e-4413-c303-102855956d52"
      },
      "source": [
        "%%timeit\n",
        "test_input_one.mm(test_input_two)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 5: 226 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWfNE_jIvS4H"
      },
      "source": [
        "# Move to GPU\n",
        "import os\n",
        "using_GPU = os.path.exists('/opt/bin/nvidia-smi')\n",
        "if using_GPU:\n",
        "  gpu_test_input_one = test_input_one.cuda()\n",
        "  gpu_test_input_two = test_input_two.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7XISOVBvZIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e53807ba-4467-4a9d-ba84-76fa2dedcf5a"
      },
      "source": [
        "%%timeit -n 100\n",
        "# This now automatically runs on the GPU!\n",
        "if using_GPU:\n",
        "  gpu_test_input_one.mm(gpu_test_input_two)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 7.66 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "100 loops, best of 5: 9.77 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AY55mDVwAEI"
      },
      "source": [
        "Using a GPU can give you massive speedups for tensor operations since most of them are easily parallelizable. Historically, the success of deep learning is inextricably tied to the ability to efficiently train the models on GPUs.\n",
        "\n",
        "To take advantage of this, **you want to be using PyTorch tensor operations almost everywhere** --- avoid explicitly iterating over tensors!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE8RE8n5cUmJ"
      },
      "source": [
        "# Computation Graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I00992FzWysI"
      },
      "source": [
        "A computation graph is simply a way to define a sequence of operations to go from input to model output. \n",
        "\n",
        "You can think of the nodes in the graph as representing operations, and the edges in the graph represent tensors going in and out.\n",
        "\n",
        "For example, say we wanted to build a linear regression model. This has the form $\\hat y = Wx + b$. \n",
        "\n",
        "In this equation, $x$ is our input, $W$ is a learned weight matrix, $b$ is a learned bias, and $\\hat y$ is the predicted output. \n",
        "\n",
        "As a computation graph, this looks like:\n",
        "\n",
        "![Linear Regression Computation Graph](https://imgur.com/IcBhTjS.png)\n",
        "\n",
        "When implementing deep learning models, you're basically designing and specifying computation graphs. It's a bit like playing with Legos in that you're stringing together a bunch of blocks (the operations) to achieve a final desired output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjkrXzQ3UDhy"
      },
      "source": [
        "# Tensors, Variables and Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF3x5x8RWUsT"
      },
      "source": [
        "One of PyTorch's key features (and what makes it a deep learning library) is the ability to specify arbitrary computation graphs and compute gradients on them automatically. For more detail, please see the official tutorial on grad at PyTorch: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\n",
        "\n",
        "We can do this on Tensor objects. In older versions of PyTorch, we need to first wrap the tensor in a `Variable` and import `torch.autograd.Variable`. You will see this in tutorials still floating around the internet, hence I wanted to mention this notation as well.  Some things you can do: \n",
        "\n",
        "*   The data of the tensor (accessed with the `.data` member)\n",
        "*   The gradient with regards to this Variable (accessed with the `.grad` member)\n",
        "*   The function that created it (accessed with the `.grad_fn` member)\n",
        "\n",
        "For legacy purposes, I want to mention that you will sometimes see this: \n",
        "\n",
        "`x = Variable(torch.Tensor([1, 2, 3]), requires_grad=True)`\n",
        "\n",
        "In newer version of TyTorch can simply use: \n",
        "\n",
        "`x = torch.Tensor([1., 2., 3.], requires_grad=True)`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP5GOtjkWTsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4722bd73-4334-454c-9822-30f3a3c97cff"
      },
      "source": [
        "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
        "# You can access the underlying tensor with the .data attribute\n",
        "print(x.data)\n",
        "\n",
        "# Any operation you could use on Tensors, you can use on the legacy Variables\n",
        "y = torch.tensor([4., 5., 6.], requires_grad=True)\n",
        "z = x + y\n",
        "print(z.data)\n",
        "\n",
        "# But z also stores where it came from!\n",
        "print(z.grad_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.])\n",
            "tensor([5., 7., 9.])\n",
            "<AddBackward0 object at 0x7fc5401fa450>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7b-U3Mfr-Px"
      },
      "source": [
        "A note on the `requires_grad` argument: with most NN code, you don’t want to set `requires_grad=True` unless you explicitly want the gradient w.r.t. to your input. In this example, however, `requires_grad=True` is necessary because otherwise there would be no gradients to compute, since there are no model parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hEy7CvTsaZw"
      },
      "source": [
        "Let's do some more operations and calculate the gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPYWL4pOsfmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66d33cd-bf23-406a-9fb6-43fb56ce775c"
      },
      "source": [
        "z_sum = torch.sum(z)\n",
        "print(z_sum)\n",
        "print(z_sum.grad_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(21., grad_fn=<SumBackward0>)\n",
            "<SumBackward0 object at 0x7fc5401fa310>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8acGLAcs2Bt"
      },
      "source": [
        "Say we want to calculate the derivative of the sum w.r.t. the \n",
        "first element of x (in math,  $\\frac{\\partial z_{sum}}{\\partial x_0}$).\n",
        "\n",
        "Autograd knows that: $$ z_{sum} = x_0 + y_0 + x_1 + y_1 + x_2 + y_2$$\n",
        "\n",
        "So the derivative of $z_{sum}$ w.r.t $x_0$ is 1! Similarily, the derivative to all elements of $x$ is 1. Let's verify this with autograd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHSS5QnxteVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563c4110-098e-4e5a-e2af-37866fc27c8c"
      },
      "source": [
        "# Backprop from s backwards into the grpah\n",
        "# It'll follow the chain of computation by going from grad_fn to grad_fn\n",
        "# until it reaches the input.\n",
        "z_sum.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2eAbqOxt2e9"
      },
      "source": [
        "Try running the block above multiple times! What do you notice happening?\n",
        "\n",
        "**The gradient in `.grad` accumulates each time we call `.backward()`** --- this is convenient for some models, but we'll usually want to zero the gradient before running backpropagation when we're training our models (more on this later).\n",
        "\n",
        "In most models we build, we'll generally want to explicitly zero-out the gradients (details forthcoming) before calling `.backward()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZNnUJcrU2r2"
      },
      "source": [
        "# Structuring PyTorch models\n",
        "\n",
        "At the highest level, `nn.Module` defines what most would refer to as a \"model\". It's a convenient way for encapsulating the trainable parameters of a model or a component of your model, and subclassing this class gives you Python functions for moving your model to the GPU, saving it, loading it etc.\n",
        "\n",
        "When you're building your own model, you're going to subclass `nn.Module`. Critically, you also need to override the `__init__()` and `forward()` functions.\n",
        "\n",
        "*   In `__init__()`, you should take arguments that modify how the model runs (e.g. # of layers, # of hidden units, output sizes). You'll also set up most of the layers that you use in the forward pass here.\n",
        "*   In `forward()`, you define the \"forward pass\" of your model, or the operations needed to transform input to output. **You can use any of the Tensor operations in the forward pass.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bxeOKmLWDly"
      },
      "source": [
        "### A simple example `Module` : Logistic Regression\n",
        "\n",
        "As a simple example of how to make a Module, let's build a logistic regression model.\n",
        "\n",
        "Logistic regression takes an input $x$ and applies a linear transform to squash the input down to a probability distribution over the number of classification classes. If you recall from the lecture, we start with a linear regression model based on the input variables, which is then put into a logistic sigmoid function. As a module, this looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4OWZ0B1zpr3"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "  # input_size: Dimensionality of input feature vector.\n",
        "  # num_classes: The number of classes in the classification problem.\n",
        "  def __init__(self, input_size, num_classes):\n",
        "    # Always call the superclass (nn.Module) constructor first!\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    # Set up the linear transform\n",
        "    self.linear = nn.Linear(input_size, num_classes)\n",
        "    # I do not yet include the sigmoid activation after the linear \n",
        "    # layer because our loss function will include this as you will see later\n",
        "\n",
        "  # Forward's sole argument is the input.\n",
        "  # input is of shape (batch_size, input_size)\n",
        "  def forward(self, x):\n",
        "    # Apply the linear transform.\n",
        "    # out is of shape (batch_size, num_classes). \n",
        "    out = self.linear(x)\n",
        "    out = torch.sigmoid(out)\n",
        "    # Softmax the out tensor to get a log-probability distribution\n",
        "    # over classes for each example.\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RZlH4EY1BIY"
      },
      "source": [
        "Modules are also callable! As a result, we can do the following to apply them to an input. Note how the number of features determines the size of the linear layer above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3n2DoVt1H8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "521da9b2-2ca3-454e-e351-59bf3a24c914"
      },
      "source": [
        "# Binary classifiation\n",
        "num_outputs = 1\n",
        "num_input_features = 2\n",
        "\n",
        "# Create the logistic regression model\n",
        "logreg_clf = LogisticRegression(num_input_features, num_outputs)\n",
        "\n",
        "print(logreg_clf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression(\n",
            "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ynjaIvgIlgk"
      },
      "source": [
        "We set a learning rate a select the gradient descent optimizer to train the model. For a super small example, we manually define a training and test set for the XOR problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcItrPEnIluy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a4ab68-13e5-404f-e249-bbda63dbd23c"
      },
      "source": [
        "import torch \n",
        "lr_rate = 0.001  # alpha\n",
        "\n",
        "# training set of input X and labels Y\n",
        "X = torch.Tensor([[0,0],[0,1], [1,0], [1,1]])\n",
        "Y = torch.Tensor([0,1,1,0]).view(-1,1) #view is similar to numpy.reshape() here it makes it into a column\n",
        "\n",
        "# Run the forward pass of the logistic regression model\n",
        "sample_output = logreg_clf(X) #completely random at the moment\n",
        "print(X)\n",
        "\n",
        "loss_function = nn.BCELoss() \n",
        "# SGD: stochastic gradient descent is used to train/fit the model\n",
        "optimizer = torch.optim.SGD(logreg_clf.parameters(), lr=lr_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 1.]])\n",
            "tensor([[0.5033],\n",
            "        [0.4767],\n",
            "        [0.5164],\n",
            "        [0.4898]], grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG1SaCwJJUCq"
      },
      "source": [
        "Now we can train!\n",
        "\n",
        "Take a moment to study what is happening here. This process will keep coming back. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd7fDCWsJUKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e35f7a4-8d8e-4081-8789-de42afcaefa5"
      },
      "source": [
        "import numpy as np \n",
        "# from torch.autograd import Variable\n",
        "\n",
        "#training loop:\n",
        "\n",
        "epochs = 2001 #how many times we go through the training set\n",
        "steps = X.size(0) #steps = 4; we have 4 training examples (I know, tiny training set :)\n",
        "\n",
        "for i in range(epochs):\n",
        "    for j in range(steps):\n",
        "        # randomly sample from the training set:\n",
        "        data_point = np.random.randint(X.size(0))\n",
        "        # store the retrieved datapoint into 2 separate variables of the right shape\n",
        "        x_var = torch.Tensor(X[data_point]) \n",
        "        y_var = torch.Tensor(Y[data_point])\n",
        "\n",
        "        # print(x_var.size())\n",
        "        \n",
        "        optimizer.zero_grad() # empty (zero) the gradient buffers\n",
        "        y_hat = logreg_clf(x_var) #get the output from the model\n",
        "\n",
        "        loss = loss_function(y_hat, y_var) #calculate the loss\n",
        "        loss.backward() #backprop\n",
        "        optimizer.step() #does the update\n",
        "\n",
        "    if i % 500 == 0:\n",
        "        print (\"Epoch: {0}, Loss: {1}, \".format(i, loss.data.numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 0.7317752242088318, \n",
            "Epoch: 500, Loss: 0.7088961601257324, \n",
            "Epoch: 1000, Loss: 0.6454429030418396, \n",
            "Epoch: 1500, Loss: 0.7373309135437012, \n",
            "Epoch: 2000, Loss: 0.6999746561050415, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHSlettsMLF9"
      },
      "source": [
        "As expected the loss remains high. XOR needs a non-linear model to work well (or, a feature engineering trick: add a third input feature: $x_1 * x_2$). Next week, we'll tackle this properly with neural networks...\n",
        "\n",
        "Below you can experiment how badly it works :)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zKUydFc3idW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "141b5fca-e65a-4ef2-cb17-45e687c906d6"
      },
      "source": [
        "test = [[0,0],[0,1],[1,1],[1,0]]\n",
        "\n",
        "for trial in test: \n",
        "  Xtest = torch.Tensor(trial)\n",
        "  y_hat = logreg_clf(Xtest)\n",
        "  \n",
        "  if y_hat > 0.5:\n",
        "    prediction = 1\n",
        "  else: \n",
        "    prediction = 0\n",
        "    \n",
        "  print(\"{0} xor {1} = {2}\".format(int(Xtest[0]), int(Xtest[1]), prediction))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 xor 0 = 1\n",
            "0 xor 1 = 0\n",
            "1 xor 1 = 0\n",
            "1 xor 0 = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iHlyGbeutty"
      },
      "source": [
        "In this week's homework you will experiment with logistic regression on a problem that is actually suited for linear separation... instead of one that requires non-linear transformations. (Don't worry, we'll solve XOR later on with a deeper neural network!)\n"
      ]
    }
  ]
}