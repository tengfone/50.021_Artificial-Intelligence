{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI homework PT2 - neural networks",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUbEmuvZJxlI"
      },
      "source": [
        "# PyTorch - homework 2: neural networks\n",
        "\n",
        "-- Prof. Dorien Herremans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efS07mO7J6AR"
      },
      "source": [
        "Please run the whole notebook with your code and submit the `.ipynb` file on eDimension that includes your answers [so after you run it]. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJpzFaX0J6Zz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacce670-0acc-4add-e60a-e16a70fc8864"
      },
      "source": [
        "from termcolor import colored\n",
        "\n",
        "student_number=\"1003296\"\n",
        "student_name=\"Phang Teng Fone\"\n",
        "\n",
        "print(colored(\"Homework by \"  + student_name + ', number: ' + student_number,'red'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mHomework by Phang Teng Fone, number: 1003296\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xDkwBg8LKQ_"
      },
      "source": [
        " ## Question 1 -- XOR neural network [3pts]\n",
        "\n",
        "a) Train an (at least) 2-layer neural network that can solve the XOR problem. Hint: be sure to check both this week and last week's lab. \n",
        "\n",
        "b) Check the predictions resulting from your model in the second code box below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BINvhm-PLKak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe35b5c-7c8d-426c-885d-48410445e6ce"
      },
      "source": [
        "# load your data\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# training set of input X and labels Y\n",
        "X = torch.Tensor([[0,0],[0,1], [1,0], [1,1]])\n",
        "Y = torch.Tensor([0,1,1,0]).view(-1,1) #view is similar to numpy.reshape() here it makes it into a column\n",
        "\n",
        "class FeedForwardNN(nn.Module):\n",
        "  # input_size: Dimensionality of input feature vector.\n",
        "  # num_classes: The number of classes in the classification problem.\n",
        "  # num_hidden: The number of hidden (intermediate) layers to use.\n",
        "  # hidden_dim: The size of each of the hidden layers.\n",
        "  # dropout: The proportion of units to drop out after each layer.\n",
        "  def __init__(self, input_size, num_classes, num_hidden, hidden_dim, dropout):\n",
        "    # Always call the superclass (nn.Module) constructor first!\n",
        "    super(FeedForwardNN, self).__init__()\n",
        "    \n",
        "    # Set up the hidden layers.\n",
        "    assert num_hidden > 0\n",
        "    # A special ModuleList to store our hidden layers.\n",
        "    self.hidden_layers = nn.ModuleList([])\n",
        "    # First hidden layer maps from input_size -> num_hidden.\n",
        "    self.hidden_layers.append(nn.Linear(input_size, hidden_dim))\n",
        "    # Subsequent hidden layers map from num_hidden -> num_hidden.\n",
        "    # Note that they can map to any dimensionality --- as long as the final\n",
        "    # output is a distribution over your classes!\n",
        "    for i in range(num_hidden - 1):\n",
        "      self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "    \n",
        "    # Set up the dropout layer.\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    # Set up the final transform to a distribution over classes.\n",
        "    self.output_projection = nn.Linear(hidden_dim, num_classes)\n",
        "    \n",
        "    # Set up the nonlinearity to use between layers.\n",
        "    self.nonlinearity = nn.ReLU()\n",
        "    \n",
        "  # Forward's sole argument is the input.\n",
        "  # input is of shape (batch_size, input_size)\n",
        "  def forward(self, x):\n",
        "    # Apply the hidden layers, nonlinearity, and dropout.\n",
        "    for hidden_layer in self.hidden_layers:\n",
        "      x = hidden_layer(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.nonlinearity(x)\n",
        "      \n",
        "    # Output layer: project x to a distribution over classes.\n",
        "    out = self.output_projection(x)\n",
        "    \n",
        "    # Softmax the out tensor to get a log-probability distribution\n",
        "    # over classes for each example.\n",
        "    out_distribution = F.log_softmax(out, dim=-1)\n",
        "    return out_distribution\n",
        "\n",
        "# name your model xor\n",
        "def xor():\n",
        "    model = FeedForwardNN(2,2,2,5,0)\n",
        "    return model\n",
        "\n",
        "model = xor()\n",
        "\n",
        "# define your model loss function, optimizer, etc.\n",
        "loss_function = nn.NLLLoss()\n",
        "momentum= 0.9\n",
        "lr_rate = 0.001  # alpha\n",
        "# SGD: stochastic gradient descent is used to train/fit the model\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate, momentum=momentum)\n",
        "\n",
        "\n",
        "# train the model\n",
        "#training loop:\n",
        "epochs = 2001 #how many times we go through the training set\n",
        "steps = X.size(0) #steps = 4; we have 4 training examples (I know, tiny training set :)\n",
        "\n",
        "for i in range(epochs):\n",
        "    for j in range(steps):        \n",
        "        optimizer.zero_grad() # empty (zero) the gradient buffers\n",
        "        y_hat = model(X[j].unsqueeze(0)) #get the output from the model\n",
        "\n",
        "        loss = loss_function(y_hat, Y[j].type(torch.LongTensor)) #calculate the loss\n",
        "        loss.backward() #backprop\n",
        "        optimizer.step() #does the update\n",
        "\n",
        "    if i % 500 == 0:\n",
        "        print (\"Epoch: {0}, Loss: {1}, \".format(i, loss.data.numpy()))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 0.5376112461090088, \n",
            "Epoch: 500, Loss: 0.36856818199157715, \n",
            "Epoch: 1000, Loss: 0.06509296596050262, \n",
            "Epoch: 1500, Loss: 0.031519174575805664, \n",
            "Epoch: 2000, Loss: 0.020460965111851692, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51Ra1T6n2r_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7abfe7ed-d034-4117-bbf1-ecce7c9a6978"
      },
      "source": [
        "# test your model using the following functions (make sure the output is printed and saved when you submit this notebook):\n",
        "# depending on how you defined your network you may need to slightly tweek the below prediction function\n",
        "\n",
        "test = [[0,0],[0,1],[1,1],[1,0]]\n",
        "\n",
        "for trial in test: \n",
        "  Xtest = torch.Tensor(trial)\n",
        "  y_hat = model(Xtest)\n",
        "\n",
        "  prediction = torch.argmax(y_hat)\n",
        "  \n",
        "  print(\"{0} xor {1} = {2}\".format(int(Xtest[0]), int(Xtest[1]), prediction))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 xor 0 = 0\n",
            "0 xor 1 = 1\n",
            "1 xor 1 = 0\n",
            "1 xor 0 = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqIqD5ZzyUOW"
      },
      "source": [
        "## Question 2  [2pts]\n",
        "\n",
        "Imagine a neural network model for a multilabel classification task. \n",
        "\n",
        "a) Which loss function should you use?\n",
        "\n",
        "b) The resulting trained modal has a high variance error. Give 4 possible solutions to improve the model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzye9G18PQ0c"
      },
      "source": [
        "```\n",
        "* answer A\n",
        "Binary cross-entropy loss function. Element base decision belonging to a certain class should not influence the decision for another class.\n",
        "* answer B\n",
        "  - Regularization\n",
        "  - Dropout\n",
        "  - Early Stopping\n",
        "  - Increase size of training set / Reduce # of model parameters\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcceOSnjjSHf"
      },
      "source": [
        "## Question 3 - Improve hit classification [5pts]\n",
        "\n",
        "Remember the hit predicton dataset from last week? \n",
        "\n",
        "a) Improve the model using a multiplayer perceptron. \n",
        "\n",
        "b) Make sure to run your models on the GPU. \n",
        "\n",
        "c) Tweek the hyperparameters such as number of nodes or layers, or other. Show two possible configurations and explain which works better and very briefly explain why this may be the case. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB-Jfw9pMDqc",
        "outputId": "a3761738-03b8-4398-a0cf-7a2186c02d48"
      },
      "source": [
        "# Download dataset\n",
        "!wget https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030training.csv\n",
        "!wget https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030test.csv"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-01 16:29:33--  https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030training.csv\n",
            "Resolving dorax.s3.ap-south-1.amazonaws.com (dorax.s3.ap-south-1.amazonaws.com)... 52.219.62.3\n",
            "Connecting to dorax.s3.ap-south-1.amazonaws.com (dorax.s3.ap-south-1.amazonaws.com)|52.219.62.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 147372 (144K) [text/csv]\n",
            "Saving to: ‘herremans_hit_1030training.csv’\n",
            "\n",
            "herremans_hit_1030t 100%[===================>] 143.92K   238KB/s    in 0.6s    \n",
            "\n",
            "2021-07-01 16:29:34 (238 KB/s) - ‘herremans_hit_1030training.csv’ saved [147372/147372]\n",
            "\n",
            "--2021-07-01 16:29:34--  https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030test.csv\n",
            "Resolving dorax.s3.ap-south-1.amazonaws.com (dorax.s3.ap-south-1.amazonaws.com)... 52.219.62.46\n",
            "Connecting to dorax.s3.ap-south-1.amazonaws.com (dorax.s3.ap-south-1.amazonaws.com)|52.219.62.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36712 (36K) [text/csv]\n",
            "Saving to: ‘herremans_hit_1030test.csv’\n",
            "\n",
            "herremans_hit_1030t 100%[===================>]  35.85K   178KB/s    in 0.2s    \n",
            "\n",
            "2021-07-01 16:29:35 (178 KB/s) - ‘herremans_hit_1030test.csv’ saved [36712/36712]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-jkJDTdjSRX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb606a1-edbb-459b-b0d3-c95d9970aa79"
      },
      "source": [
        "# code your model 1\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# load data\n",
        "train_pd = pd.read_csv('./herremans_hit_1030training.csv')\n",
        "labels = train_pd.iloc[:,-1]\n",
        "features = train_pd.loc[:, 'timesignature':'T12kurtosis']\n",
        "\n",
        "labels = torch.Tensor(labels.values).reshape(321,1)\n",
        "features = torch.Tensor(features.values)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# define model 1\n",
        "class Model1(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_size):\n",
        "        super(Model1, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim,hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size,hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size,output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.linear3(x)\n",
        "        x = nn.functional.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# train model\n",
        "input_dim = features.size(1)\n",
        "output_dim = 1\n",
        "hidden_size = 20\n",
        "num_of_data = features.size(0)\n",
        "epochs = 100\n",
        "lr = 0.01\n",
        "loss_func = torch.nn.BCELoss()\n",
        "\n",
        "model_1 = Model1(input_dim,output_dim,hidden_size).to(device)\n",
        "optimizer = torch.optim.SGD(model_1.parameters(), lr = lr)\n",
        "\n",
        "\n",
        "model_1.train()\n",
        "\n",
        "for i in range(epochs):\n",
        "    for j in range(num_of_data):\n",
        "        # randomly sample from the training set:\n",
        "        data_point = np.random.randint(num_of_data)\n",
        "        # store the retrieved datapoint into 2 separate variables of the right shape\n",
        "        x_var = torch.Tensor(features[data_point]).unsqueeze(0).cuda()\n",
        "        y_var = torch.Tensor(labels[data_point]).unsqueeze(0).cuda()\n",
        "\n",
        "        # print(x_var.size())\n",
        "        \n",
        "        optimizer.zero_grad() # empty (zero) the gradient buffers\n",
        "        y_hat = model_1(x_var) #get the output from the model\n",
        "\n",
        "        loss = loss_func(y_hat, y_var) #calculate the loss\n",
        "        loss.backward() #backprop\n",
        "        optimizer.step() #does the update\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Epoch - {i} , Loss - {loss.item()}\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch - 0 , Loss - 0.48464882373809814\n",
            "Epoch - 10 , Loss - 0.6198206543922424\n",
            "Epoch - 20 , Loss - 0.00617109565064311\n",
            "Epoch - 30 , Loss - 7.152560215217818e-07\n",
            "Epoch - 40 , Loss - 2.3841860752327193e-07\n",
            "Epoch - 50 , Loss - 0.002638539532199502\n",
            "Epoch - 60 , Loss - 1.1920930376163597e-07\n",
            "Epoch - 70 , Loss - 0.002432978944852948\n",
            "Epoch - 80 , Loss - 0.0\n",
            "Epoch - 90 , Loss - 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIDPTKcFkETc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4fc53c-8fb1-4fe7-ea32-9ee38141079e"
      },
      "source": [
        "# evaluate model 1 (called model1 here)\n",
        "def run_evaluation(my_model):\n",
        "\n",
        "  test = pd.read_csv('/content/herremans_hit_1030test.csv')\n",
        "  labels = test.iloc[:,-1]\n",
        "  test = test.drop('Topclass1030', axis=1)\n",
        "  testdata = torch.Tensor(test.values)\n",
        "  testlabels = torch.Tensor(labels.values).view(-1,1)\n",
        "\n",
        "  TP = 0\n",
        "  TN = 0\n",
        "  FN = 0\n",
        "  FP = 0\n",
        "\n",
        "  for i in range(0, testdata.size()[0]): \n",
        "    # print(testdata[i].size())\n",
        "    Xtest = torch.Tensor(testdata[i]).cuda()\n",
        "    y_hat = my_model(Xtest)\n",
        "    \n",
        "    if y_hat > 0.5:\n",
        "      prediction = 1\n",
        "    else: \n",
        "      prediction = 0\n",
        "\n",
        "    if (prediction == testlabels[i]):\n",
        "      if (prediction == 1):\n",
        "        TP += 1\n",
        "      else: \n",
        "        TN += 1\n",
        "\n",
        "    else:\n",
        "      if (prediction == 1):\n",
        "        FP += 1\n",
        "      else: \n",
        "        FN += 1\n",
        "\n",
        "  print(\"True Positives: {0}, True Negatives: {1}\".format(TP, TN))\n",
        "  print(\"False Positives: {0}, False Negatives: {1}\".format(FP, FN))\n",
        "  rate = TP/(FN+TP)\n",
        "  print(\"Class specific accuracy of correctly predicting a hit song is {0}\".format(rate))\n",
        "\n",
        "run_evaluation(model_1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Positives: 38, True Negatives: 18\n",
            "False Positives: 11, False Negatives: 12\n",
            "Class specific accuracy of correctly predicting a hit song is 0.76\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xghPDDNmkHn2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e5777f-8d91-403e-c232-9205a53b2599"
      },
      "source": [
        "# code your model 2\n",
        "# define model 2 \n",
        "class Model2(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_size):\n",
        "        super(Model2, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim,hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size,output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = nn.functional.dropout(x,p=0.5)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = nn.functional.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# train model\n",
        "input_dim = features.size(1)\n",
        "output_dim = 1\n",
        "hidden_size = 100\n",
        "num_of_data = features.size(0)\n",
        "epochs = 100\n",
        "lr = 0.01\n",
        "loss_func = torch.nn.BCELoss()\n",
        "\n",
        "model_2 = Model2(input_dim,output_dim,hidden_size).to(device)\n",
        "optimizer = torch.optim.SGD(model_1.parameters(), lr = lr)\n",
        "\n",
        "\n",
        "model_2.train()\n",
        "\n",
        "for i in range(epochs):\n",
        "    for j in range(num_of_data):\n",
        "        # randomly sample from the training set:\n",
        "        data_point = np.random.randint(num_of_data)\n",
        "        # store the retrieved datapoint into 2 separate variables of the right shape\n",
        "        x_var = torch.Tensor(features[data_point]).unsqueeze(0).cuda()\n",
        "        y_var = torch.Tensor(labels[data_point]).unsqueeze(0).cuda()\n",
        "\n",
        "        # print(x_var.size())\n",
        "        \n",
        "        optimizer.zero_grad() # empty (zero) the gradient buffers\n",
        "        y_hat = model_1(x_var) #get the output from the model\n",
        "\n",
        "        loss = loss_func(y_hat, y_var) #calculate the loss\n",
        "        loss.backward() #backprop\n",
        "        optimizer.step() #does the update\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Epoch - {i} , Loss - {loss.item()}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch - 0 , Loss - 1.7285496141994372e-05\n",
            "Epoch - 10 , Loss - 0.0009030603687278926\n",
            "Epoch - 20 , Loss - 4.3333515350241214e-05\n",
            "Epoch - 30 , Loss - 0.00013870962720829993\n",
            "Epoch - 40 , Loss - 0.0002667663502506912\n",
            "Epoch - 50 , Loss - 3.4570753086882178e-06\n",
            "Epoch - 60 , Loss - 0.0002977695257868618\n",
            "Epoch - 70 , Loss - 0.0006927266367711127\n",
            "Epoch - 80 , Loss - 8.702316335984506e-06\n",
            "Epoch - 90 , Loss - 0.0002803599345497787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAIifiHJkHyW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbaa0b61-10cd-4914-bf4e-990562612596"
      },
      "source": [
        "# evaluate model 2 (called model2 here)\n",
        "\n",
        "run_evaluation(model_2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Positives: 42, True Negatives: 6\n",
            "False Positives: 23, False Negatives: 8\n",
            "Class specific accuracy of correctly predicting a hit song is 0.84\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPsxbT0KkGs1"
      },
      "source": [
        "Which works better and why do you think this may be (very briefly)? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GzjI77HkSwH"
      },
      "source": [
        "For both Model 1 and 2, it uses the same learning rate, epochs and loss functions. The only differences are\n",
        "\n",
        "Model 1:\n",
        "- Uses 3 hidden layers of size 20\n",
        "\n",
        "Model 2:\n",
        "- Uses 2 hidden layer of size 100\n",
        "- An additional drop out layer to prevent overfitting\n",
        "\n",
        "Model 2 works better as adding drop out layer prevents overfitting, additional features of hidden layers reduce training loss and reducing hidden layers might improve variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh5O8qS_khug"
      },
      "source": [
        "Additionally, submit your results [here](https://forms.gle/NtJJEE7Wm5ZRM3Je7) for 'Class specific accuracy of correctly predicting a hit song' and see if you got the best performance of the class! Good luck!"
      ]
    }
  ]
}